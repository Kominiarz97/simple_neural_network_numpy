{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_neural_network_numpy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNeshkR1RLaz4ov8NoKDvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kominiarz97/simple_neural_network_numpy/blob/master/simple_neural_network_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DU_pYFePUe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X=np.array([1.4,0.7])\n",
        "y_true=np.array([1.8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzJJr7weU2vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_param(n_in,n_h,n_out):\n",
        "    W1=np.random.rand(n_h,n_in)\n",
        "    W2=np.random.rand(n_h,n_out)\n",
        "    return W1,W2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Aon4WkWain",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forw_prop(X,W1,W2):\n",
        "    H1=np.dot(X,W1)\n",
        "    y_pred=np.dot(H1,W2)\n",
        "    return H1,y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNBK-dq-XGib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def error_calc(y_true,y_pred):\n",
        "    error=y_pred-y_true\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz33tERSXSg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(X,W1,W2):\n",
        "    _,y_pred=forw_prop(X,W1,W2)\n",
        "    return y_pred[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ4BfUOQc0ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_prop(X,W1,W2,learning_rate,iterations=1000,precision=0.0000001):\n",
        "    H1,y_pred=forw_prop(X,W1,W2)\n",
        "    train_loss=[]\n",
        "    for i in range(iterations):\n",
        "        error=error_calc(y_true,y_pred)\n",
        "        W2=W2-learning_rate*error*H1.T\n",
        "        W1=W1-learning_rate*error*np.dot(X.T,W2.T)\n",
        "        y_pred=pred(X,W1,W2)\n",
        "        loss=abs(error_calc(y_pred,y_true[0]))\n",
        "        print(f'Iteration {i}: y_pred: {y_pred}, loss: {loss}')\n",
        "        train_loss.append(loss)\n",
        "        if abs(error)<precision:\n",
        "            break\n",
        "    return W1,W2,train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhzfAkhCfYyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    W1,W2=init_param(2,2,1)\n",
        "    W1,W2,train_loss=back_prop(X,W1,W2,0.01)\n",
        "    model={'W1':W1,'W2':W2,'train_loss':train_loss}\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz4IIjJNgISJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fe86d4a-841b-47dd-b121-dbe9c34f403d"
      },
      "source": [
        "model=build_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0: y_pred: 0.47775082418068876, loss: 1.3222491758193113\n",
            "Iteration 1: y_pred: 0.5203197164995874, loss: 1.2796802835004126\n",
            "Iteration 2: y_pred: 0.5624603087798719, loss: 1.2375396912201282\n",
            "Iteration 3: y_pred: 0.6041630898849911, loss: 1.195836910115009\n",
            "Iteration 4: y_pred: 0.6454096471221384, loss: 1.1545903528778616\n",
            "Iteration 5: y_pred: 0.686174266816854, loss: 1.113825733183146\n",
            "Iteration 6: y_pred: 0.7264254411736586, loss: 1.0735745588263415\n",
            "Iteration 7: y_pred: 0.7661272789328608, loss: 1.0338727210671392\n",
            "Iteration 8: y_pred: 0.8052408159236343, loss: 0.9947591840763658\n",
            "Iteration 9: y_pred: 0.8437252210745323, loss: 0.9562747789254677\n",
            "Iteration 10: y_pred: 0.8815388936200005, loss: 0.9184611063799996\n",
            "Iteration 11: y_pred: 0.9186404479839603, loss: 0.8813595520160398\n",
            "Iteration 12: y_pred: 0.9549895839833127, loss: 0.8450104160166874\n",
            "Iteration 13: y_pred: 0.9905478414374655, loss: 0.8094521585625345\n",
            "Iteration 14: y_pred: 1.025279239868215, loss: 0.7747207601317851\n",
            "Iteration 15: y_pred: 1.0591508056150682, loss: 0.7408491943849318\n",
            "Iteration 16: y_pred: 1.0921329902786125, loss: 0.7078670097213875\n",
            "Iteration 17: y_pred: 1.1241999858610763, loss: 0.6758000141389238\n",
            "Iteration 18: y_pred: 1.1553299432399835, loss: 0.6446700567600165\n",
            "Iteration 19: y_pred: 1.185505101647381, loss: 0.6144948983526191\n",
            "Iteration 20: y_pred: 1.2147118376107076, loss: 0.5852881623892925\n",
            "Iteration 21: y_pred: 1.2429406423347058, loss: 0.5570593576652942\n",
            "Iteration 22: y_pred: 1.2701860367729518, loss: 0.5298139632270482\n",
            "Iteration 23: y_pred: 1.296446433669521, loss: 0.5035535663304791\n",
            "Iteration 24: y_pred: 1.3217239556708602, loss: 0.47827604432913984\n",
            "Iteration 25: y_pred: 1.346024218245523, loss: 0.45397578175447695\n",
            "Iteration 26: y_pred: 1.3693560856383367, loss: 0.4306439143616634\n",
            "Iteration 27: y_pred: 1.3917314074600646, loss: 0.4082685925399354\n",
            "Iteration 28: y_pred: 1.4131647428070897, loss: 0.38683525719291034\n",
            "Iteration 29: y_pred: 1.4336730780491806, loss: 0.3663269219508194\n",
            "Iteration 30: y_pred: 1.4532755436450078, loss: 0.34672445635499227\n",
            "Iteration 31: y_pred: 1.4719931345688555, loss: 0.32800686543114455\n",
            "Iteration 32: y_pred: 1.489848438177975, loss: 0.31015156182202497\n",
            "Iteration 33: y_pred: 1.5068653726340866, loss: 0.29313462736591345\n",
            "Iteration 34: y_pred: 1.5230689383266065, loss: 0.27693106167339354\n",
            "Iteration 35: y_pred: 1.5384849841373773, loss: 0.2615150158626227\n",
            "Iteration 36: y_pred: 1.5531399898420248, loss: 0.2468600101579752\n",
            "Iteration 37: y_pred: 1.567060865463699, loss: 0.232939134536301\n",
            "Iteration 38: y_pred: 1.5802747679806677, loss: 0.21972523201933236\n",
            "Iteration 39: y_pred: 1.5928089354380996, loss: 0.2071910645619004\n",
            "Iteration 40: y_pred: 1.6046905382229744, loss: 0.19530946177702568\n",
            "Iteration 41: y_pred: 1.6159465470249987, loss: 0.1840534529750013\n",
            "Iteration 42: y_pred: 1.6266036168207822, loss: 0.1733963831792178\n",
            "Iteration 43: y_pred: 1.63668798607784, loss: 0.16331201392216\n",
            "Iteration 44: y_pred: 1.6462253902739443, loss: 0.15377460972605572\n",
            "Iteration 45: y_pred: 1.6552409887604627, loss: 0.14475901123953738\n",
            "Iteration 46: y_pred: 1.6637593039605378, loss: 0.1362406960394622\n",
            "Iteration 47: y_pred: 1.6718041718794512, loss: 0.12819582812054886\n",
            "Iteration 48: y_pred: 1.6793987029108668, loss: 0.12060129708913325\n",
            "Iteration 49: y_pred: 1.6865652519449181, loss: 0.11343474805508191\n",
            "Iteration 50: y_pred: 1.6933253968187492, loss: 0.10667460318125088\n",
            "Iteration 51: y_pred: 1.6996999241940762, loss: 0.10030007580592382\n",
            "Iteration 52: y_pred: 1.7057088219969534, loss: 0.09429117800304665\n",
            "Iteration 53: y_pred: 1.7113712776099892, loss: 0.08862872239001085\n",
            "Iteration 54: y_pred: 1.7167056810648456, loss: 0.08329431893515449\n",
            "Iteration 55: y_pred: 1.7217296325414804, loss: 0.07827036745851967\n",
            "Iteration 56: y_pred: 1.7264599535389993, loss: 0.07354004646100076\n",
            "Iteration 57: y_pred: 1.7309127011401986, loss: 0.06908729885980147\n",
            "Iteration 58: y_pred: 1.7351031848471579, loss: 0.06489681515284218\n",
            "Iteration 59: y_pred: 1.7390459855180398, loss: 0.06095401448196025\n",
            "Iteration 60: y_pred: 1.7427549759851644, loss: 0.057245024014835666\n",
            "Iteration 61: y_pred: 1.7462433429812245, loss: 0.05375665701877552\n",
            "Iteration 62: y_pred: 1.7495236100440095, loss: 0.050476389955990575\n",
            "Iteration 63: y_pred: 1.7526076611102193, loss: 0.04739233888978078\n",
            "Iteration 64: y_pred: 1.7555067645458284, loss: 0.04449323545417161\n",
            "Iteration 65: y_pred: 1.7582315973941327, loss: 0.04176840260586734\n",
            "Iteration 66: y_pred: 1.7607922696531622, loss: 0.03920773034683789\n",
            "Iteration 67: y_pred: 1.7631983484217237, loss: 0.036801651578276307\n",
            "Iteration 68: y_pred: 1.7654588817781245, loss: 0.034541118221875555\n",
            "Iteration 69: y_pred: 1.7675824222777847, loss: 0.03241757772221532\n",
            "Iteration 70: y_pred: 1.7695770499756733, loss: 0.03042295002432671\n",
            "Iteration 71: y_pred: 1.771450394896963, loss: 0.028549605103036946\n",
            "Iteration 72: y_pred: 1.7732096588947255, loss: 0.026790341105274562\n",
            "Iteration 73: y_pred: 1.774861636846979, loss: 0.025138363153021093\n",
            "Iteration 74: y_pred: 1.7764127371572318, loss: 0.023587262842768242\n",
            "Iteration 75: y_pred: 1.7778690015329075, loss: 0.022130998467092544\n",
            "Iteration 76: y_pred: 1.7792361240248984, loss: 0.020763875975101653\n",
            "Iteration 77: y_pred: 1.7805194693191018, loss: 0.019480530680898278\n",
            "Iteration 78: y_pred: 1.7817240902772846, loss: 0.018275909722715422\n",
            "Iteration 79: y_pred: 1.7828547447301206, loss: 0.017145255269879422\n",
            "Iteration 80: y_pred: 1.7839159115298464, loss: 0.016084088470153635\n",
            "Iteration 81: y_pred: 1.784911805873826, loss: 0.015088194126174015\n",
            "Iteration 82: y_pred: 1.785846393913453, loss: 0.014153606086547033\n",
            "Iteration 83: y_pred: 1.7867234066653805, loss: 0.013276593334619502\n",
            "Iteration 84: y_pred: 1.7875463532440663, loss: 0.012453646755933745\n",
            "Iteration 85: y_pred: 1.7883185334362186, loss: 0.01168146656378144\n",
            "Iteration 86: y_pred: 1.7890430496388665, loss: 0.010956950361133533\n",
            "Iteration 87: y_pred: 1.7897228181836415, loss: 0.010277181816358505\n",
            "Iteration 88: y_pred: 1.7903605800703621, loss: 0.009639419929637905\n",
            "Iteration 89: y_pred: 1.7909589111333402, loss: 0.009041088866659885\n",
            "Iteration 90: y_pred: 1.7915202316638739, loss: 0.008479768336126181\n",
            "Iteration 91: y_pred: 1.7920468155123246, loss: 0.007953184487675458\n",
            "Iteration 92: y_pred: 1.7925407986929023, loss: 0.0074592013070977625\n",
            "Iteration 93: y_pred: 1.7930041875139482, loss: 0.006995812486051856\n",
            "Iteration 94: y_pred: 1.79343886625601, loss: 0.006561133743990144\n",
            "Iteration 95: y_pred: 1.7938466044194779, loss: 0.006153395580522192\n",
            "Iteration 96: y_pred: 1.7942290635629394, loss: 0.005770936437060614\n",
            "Iteration 97: y_pred: 1.7945878037527445, loss: 0.00541219624725553\n",
            "Iteration 98: y_pred: 1.7949242896435964, loss: 0.00507571035640364\n",
            "Iteration 99: y_pred: 1.7952398962092577, loss: 0.0047601037907423205\n",
            "Iteration 100: y_pred: 1.7955359141417324, loss: 0.00446408585826763\n",
            "Iteration 101: y_pred: 1.795813554936549, loss: 0.004186445063451005\n",
            "Iteration 102: y_pred: 1.7960739556810235, loss: 0.003926044318976585\n",
            "Iteration 103: y_pred: 1.7963181835616495, loss: 0.003681816438350527\n",
            "Iteration 104: y_pred: 1.7965472401060394, loss: 0.003452759893960655\n",
            "Iteration 105: y_pred: 1.796762065174116, loss: 0.00323793482588397\n",
            "Iteration 106: y_pred: 1.7969635407125617, loss: 0.0030364592874383423\n",
            "Iteration 107: y_pred: 1.7971524942858603, loss: 0.0028475057141397198\n",
            "Iteration 108: y_pred: 1.7973297023965797, loss: 0.002670297603420302\n",
            "Iteration 109: y_pred: 1.7974958936069383, loss: 0.0025041063930617558\n",
            "Iteration 110: y_pred: 1.7976517514730472, loss: 0.002348248526952812\n",
            "Iteration 111: y_pred: 1.7977979173026548, loss: 0.002202082697345231\n",
            "Iteration 112: y_pred: 1.7979349927466213, loss: 0.00206500725337877\n",
            "Iteration 113: y_pred: 1.7980635422338285, loss: 0.0019364577661715732\n",
            "Iteration 114: y_pred: 1.798184095258689, loss: 0.001815904741311103\n",
            "Iteration 115: y_pred: 1.7982971485299188, loss: 0.0017028514700812014\n",
            "Iteration 116: y_pred: 1.7984031679887722, loss: 0.0015968320112278445\n",
            "Iteration 117: y_pred: 1.798502590704461, loss: 0.0014974092955390983\n",
            "Iteration 118: y_pred: 1.7985958266540703, loss: 0.0014041733459297934\n",
            "Iteration 119: y_pred: 1.7986832603938423, loss: 0.0013167396061577463\n",
            "Iteration 120: y_pred: 1.7987652526283422, loss: 0.0012347473716578516\n",
            "Iteration 121: y_pred: 1.7988421416836093, loss: 0.001157858316390703\n",
            "Iteration 122: y_pred: 1.7989142448900772, loss: 0.0010857551099228147\n",
            "Iteration 123: y_pred: 1.7989818598806964, loss: 0.0010181401193036788\n",
            "Iteration 124: y_pred: 1.7990452658093747, loss: 0.000954734190625306\n",
            "Iteration 125: y_pred: 1.7991047244945615, loss: 0.000895275505438553\n",
            "Iteration 126: y_pred: 1.799160481492506, loss: 0.0008395185074940859\n",
            "Iteration 127: y_pred: 1.7992127671044686, loss: 0.0007872328955313979\n",
            "Iteration 128: y_pred: 1.7992617973218947, loss: 0.0007382026781053153\n",
            "Iteration 129: y_pred: 1.799307774713335, loss: 0.0006922252866650158\n",
            "Iteration 130: y_pred: 1.799350889256658, loss: 0.0006491107433419518\n",
            "Iteration 131: y_pred: 1.7993913191199102, loss: 0.0006086808800898069\n",
            "Iteration 132: y_pred: 1.7994292313939488, loss: 0.0005707686060512085\n",
            "Iteration 133: y_pred: 1.7994647827798116, loss: 0.0005352172201884553\n",
            "Iteration 134: y_pred: 1.7994981202335922, loss: 0.000501879766407809\n",
            "Iteration 135: y_pred: 1.799529381571432, loss: 0.00047061842856810365\n",
            "Iteration 136: y_pred: 1.799558696037076, loss: 0.0004413039629240778\n",
            "Iteration 137: y_pred: 1.7995861848342956, loss: 0.00041381516570448973\n",
            "Iteration 138: y_pred: 1.7996119616263373, loss: 0.00038803837366274685\n",
            "Iteration 139: y_pred: 1.7996361330044326, loss: 0.0003638669955674523\n",
            "Iteration 140: y_pred: 1.7996587989272692, loss: 0.00034120107273083455\n",
            "Iteration 141: y_pred: 1.7996800531332222, loss: 0.0003199468667778316\n",
            "Iteration 142: y_pred: 1.799699983527016, loss: 0.00030001647298405487\n",
            "Iteration 143: y_pred: 1.7997186725424064, loss: 0.00028132745759368305\n",
            "Iteration 144: y_pred: 1.799736197482355, loss: 0.00026380251764512863\n",
            "Iteration 145: y_pred: 1.7997526308380936, loss: 0.0002473691619064855\n",
            "Iteration 146: y_pred: 1.7997680405883811, loss: 0.00023195941161890943\n",
            "Iteration 147: y_pred: 1.799782490480178, loss: 0.00021750951982202338\n",
            "Iteration 148: y_pred: 1.7997960402918907, loss: 0.00020395970810938024\n",
            "Iteration 149: y_pred: 1.7998087460802608, loss: 0.00019125391973928707\n",
            "Iteration 150: y_pred: 1.799820660411923, loss: 0.00017933958807714312\n",
            "Iteration 151: y_pred: 1.7998318325805622, loss: 0.00016816741943781466\n",
            "Iteration 152: y_pred: 1.7998423088105868, loss: 0.0001576911894132227\n",
            "Iteration 153: y_pred: 1.7998521324481316, loss: 0.00014786755186846356\n",
            "Iteration 154: y_pred: 1.7998613441401952, loss: 0.0001386558598048815\n",
            "Iteration 155: y_pred: 1.7998699820026307, loss: 0.00013001799736933606\n",
            "Iteration 156: y_pred: 1.7998780817776976, loss: 0.00012191822230245286\n",
            "Iteration 157: y_pred: 1.7998856769818121, loss: 0.0001143230181879229\n",
            "Iteration 158: y_pred: 1.799892799044106, loss: 0.00010720095589400458\n",
            "Iteration 159: y_pred: 1.799899477436369, loss: 0.00010052256363102252\n",
            "Iteration 160: y_pred: 1.7999057397949083, loss: 9.426020509173405e-05\n",
            "Iteration 161: y_pred: 1.7999116120348222, loss: 8.838796517784964e-05\n",
            "Iteration 162: y_pred: 1.7999171184571692, loss: 8.288154283087046e-05\n",
            "Iteration 163: y_pred: 1.799922281849463, loss: 7.77181505371427e-05\n",
            "Iteration 164: y_pred: 1.7999271235799168, loss: 7.287642008324546e-05\n",
            "Iteration 165: y_pred: 1.7999316636858218, loss: 6.833631417824115e-05\n",
            "Iteration 166: y_pred: 1.7999359209564247, loss: 6.40790435753047e-05\n",
            "Iteration 167: y_pred: 1.7999399130106464, loss: 6.0086989353669296e-05\n",
            "Iteration 168: y_pred: 1.7999436563699636, loss: 5.6343630036481684e-05\n",
            "Iteration 169: y_pred: 1.7999471665267532, loss: 5.2833473246804985e-05\n",
            "Iteration 170: y_pred: 1.7999504580083818, loss: 4.954199161821826e-05\n",
            "Iteration 171: y_pred: 1.7999535444373063, loss: 4.645556269378126e-05\n",
            "Iteration 172: y_pred: 1.7999564385874307, loss: 4.356141256933732e-05\n",
            "Iteration 173: y_pred: 1.7999591524369567, loss: 4.084756304334469e-05\n",
            "Iteration 174: y_pred: 1.7999616972179413, loss: 3.830278205874116e-05\n",
            "Iteration 175: y_pred: 1.7999640834627697, loss: 3.5916537230340495e-05\n",
            "Iteration 176: y_pred: 1.7999663210477341, loss: 3.367895226591422e-05\n",
            "Iteration 177: y_pred: 1.7999684192338994, loss: 3.158076610065841e-05\n",
            "Iteration 178: y_pred: 1.7999703867054242, loss: 2.9613294575847604e-05\n",
            "Iteration 179: y_pred: 1.7999722316054934, loss: 2.7768394506688665e-05\n",
            "Iteration 180: y_pred: 1.7999739615700154, loss: 2.603842998460948e-05\n",
            "Iteration 181: y_pred: 1.7999755837592204, loss: 2.4416240779645548e-05\n",
            "Iteration 182: y_pred: 1.7999771048872897, loss: 2.289511271036382e-05\n",
            "Iteration 183: y_pred: 1.7999785312501402, loss: 2.1468749859865355e-05\n",
            "Iteration 184: y_pred: 1.7999798687514794, loss: 2.013124852062731e-05\n",
            "Iteration 185: y_pred: 1.7999811229272353, loss: 1.88770727647114e-05\n",
            "Iteration 186: y_pred: 1.799982298968469, loss: 1.7701031530981126e-05\n",
            "Iteration 187: y_pred: 1.7999834017428533, loss: 1.6598257146727136e-05\n",
            "Iteration 188: y_pred: 1.7999844358148207, loss: 1.5564185179339773e-05\n",
            "Iteration 189: y_pred: 1.7999854054644528, loss: 1.4594535547196585e-05\n",
            "Iteration 190: y_pred: 1.799986314705194, loss: 1.368529480605396e-05\n",
            "Iteration 191: y_pred: 1.799987167300459, loss: 1.283269954099886e-05\n",
            "Iteration 192: y_pred: 1.799987966779212, loss: 1.2033220788021382e-05\n",
            "Iteration 193: y_pred: 1.799988716450569, loss: 1.1283549431029272e-05\n",
            "Iteration 194: y_pred: 1.7999894194174964, loss: 1.0580582503694203e-05\n",
            "Iteration 195: y_pred: 1.7999900785896479, loss: 9.921410352164983e-06\n",
            "Iteration 196: y_pred: 1.7999906966954144, loss: 9.303304585595029e-06\n",
            "Iteration 197: y_pred: 1.79999127629321, loss: 8.723706790059182e-06\n",
            "Iteration 198: y_pred: 1.799991819782063, loss: 8.180217937026057e-06\n",
            "Iteration 199: y_pred: 1.799992329411547, loss: 7.670588453079219e-06\n",
            "Iteration 200: y_pred: 1.7999928072910878, loss: 7.192708912251433e-06\n",
            "Iteration 201: y_pred: 1.7999932553986981, loss: 6.744601301900133e-06\n",
            "Iteration 202: y_pred: 1.799993675589161, loss: 6.324410839031458e-06\n",
            "Iteration 203: y_pred: 1.799994069601709, loss: 5.93039829110964e-06\n",
            "Iteration 204: y_pred: 1.799994439067222, loss: 5.560932778037042e-06\n",
            "Iteration 205: y_pred: 1.7999947855149756, loss: 5.214485024440663e-06\n",
            "Iteration 206: y_pred: 1.7999951103789753, loss: 4.889621024739554e-06\n",
            "Iteration 207: y_pred: 1.7999954150038868, loss: 4.584996113221607e-06\n",
            "Iteration 208: y_pred: 1.799995700650605, loss: 4.2993493949428085e-06\n",
            "Iteration 209: y_pred: 1.799995968501471, loss: 4.0314985290113015e-06\n",
            "Iteration 210: y_pred: 1.7999962196651662, loss: 3.7803348338361076e-06\n",
            "Iteration 211: y_pred: 1.7999964551813026, loss: 3.5448186974651463e-06\n",
            "Iteration 212: y_pred: 1.7999966760247244, loss: 3.3239752756930585e-06\n",
            "Iteration 213: y_pred: 1.799996883109542, loss: 3.116890457954824e-06\n",
            "Iteration 214: y_pred: 1.7999970772929192, loss: 2.922707080799114e-06\n",
            "Iteration 215: y_pred: 1.7999972593786162, loss: 2.740621383834352e-06\n",
            "Iteration 216: y_pred: 1.7999974301203214, loss: 2.5698796786155498e-06\n",
            "Iteration 217: y_pred: 1.799997590224767, loss: 2.4097752331364575e-06\n",
            "Iteration 218: y_pred: 1.799997740354656, loss: 2.2596453439494013e-06\n",
            "Iteration 219: y_pred: 1.7999978811314055, loss: 2.1188685945805474e-06\n",
            "Iteration 220: y_pred: 1.7999980131377178, loss: 1.9868622822549753e-06\n",
            "Iteration 221: y_pred: 1.7999981369199936, loss: 1.8630800064922681e-06\n",
            "Iteration 222: y_pred: 1.799998252990592, loss: 1.747009408026301e-06\n",
            "Iteration 223: y_pred: 1.799998361829952, loss: 1.6381700480572192e-06\n",
            "Iteration 224: y_pred: 1.7999984638885822, loss: 1.5361114178435997e-06\n",
            "Iteration 225: y_pred: 1.7999985595889243, loss: 1.4404110757482158e-06\n",
            "Iteration 226: y_pred: 1.799998649327101, loss: 1.3506728990808625e-06\n",
            "Iteration 227: y_pred: 1.7999987334745566, loss: 1.2665254434107709e-06\n",
            "Iteration 228: y_pred: 1.7999988123795947, loss: 1.187620405351808e-06\n",
            "Iteration 229: y_pred: 1.79999888636882, loss: 1.1136311799386789e-06\n",
            "Iteration 230: y_pred: 1.7999989557484894, loss: 1.0442515105957284e-06\n",
            "Iteration 231: y_pred: 1.799999020805779, loss: 9.791942210402027e-07\n",
            "Iteration 232: y_pred: 1.7999990818099745, loss: 9.181900255672559e-07\n",
            "Iteration 233: y_pred: 1.7999991390135857, loss: 8.609864143860335e-07\n",
            "Iteration 234: y_pred: 1.7999991926533894, loss: 8.073466106761629e-07\n",
            "Iteration 235: y_pred: 1.799999242951413, loss: 7.570485871521981e-07\n",
            "Iteration 236: y_pred: 1.7999992901158497, loss: 7.098841503516695e-07\n",
            "Iteration 237: y_pred: 1.7999993343419238, loss: 6.656580762154363e-07\n",
            "Iteration 238: y_pred: 1.7999993758126962, loss: 6.241873038437262e-07\n",
            "Iteration 239: y_pred: 1.7999994146998226, loss: 5.853001774358546e-07\n",
            "Iteration 240: y_pred: 1.7999994511642656, loss: 5.488357344152206e-07\n",
            "Iteration 241: y_pred: 1.7999994853569592, loss: 5.146430408498048e-07\n",
            "Iteration 242: y_pred: 1.799999517419434, loss: 4.825805659525173e-07\n",
            "Iteration 243: y_pred: 1.7999995474844042, loss: 4.5251559588344037e-07\n",
            "Iteration 244: y_pred: 1.7999995756763145, loss: 4.243236855216992e-07\n",
            "Iteration 245: y_pred: 1.7999996021118574, loss: 3.978881426558445e-07\n",
            "Iteration 246: y_pred: 1.7999996269004548, loss: 3.7309954525888145e-07\n",
            "Iteration 247: y_pred: 1.7999996501447126, loss: 3.4985528740705263e-07\n",
            "Iteration 248: y_pred: 1.7999996719408435, loss: 3.280591565069102e-07\n",
            "Iteration 249: y_pred: 1.7999996923790667, loss: 3.0762093339298247e-07\n",
            "Iteration 250: y_pred: 1.7999997115439799, loss: 2.88456020181016e-07\n",
            "Iteration 251: y_pred: 1.7999997295149113, loss: 2.704850887713661e-07\n",
            "Iteration 252: y_pred: 1.7999997463662463, loss: 2.536337537772937e-07\n",
            "Iteration 253: y_pred: 1.7999997621677362, loss: 2.378322638829644e-07\n",
            "Iteration 254: y_pred: 1.7999997769847864, loss: 2.2301521362955157e-07\n",
            "Iteration 255: y_pred: 1.7999997908787284, loss: 2.091212716326396e-07\n",
            "Iteration 256: y_pred: 1.7999998039070721, loss: 1.9609292789546373e-07\n",
            "Iteration 257: y_pred: 1.7999998161237443, loss: 1.8387625577709343e-07\n",
            "Iteration 258: y_pred: 1.7999998275793128, loss: 1.724206872832923e-07\n",
            "Iteration 259: y_pred: 1.7999998383211948, loss: 1.6167880523276779e-07\n",
            "Iteration 260: y_pred: 1.7999998483938526, loss: 1.5160614741382972e-07\n",
            "Iteration 261: y_pred: 1.7999998578389798, loss: 1.421610202889667e-07\n",
            "Iteration 262: y_pred: 1.7999998666956718, loss: 1.33304328242545e-07\n",
            "Iteration 263: y_pred: 1.7999998750005881, loss: 1.249994119323361e-07\n",
            "Iteration 264: y_pred: 1.7999998827881043, loss: 1.1721189574487312e-07\n",
            "Iteration 265: y_pred: 1.7999998900904552, loss: 1.0990954479872528e-07\n",
            "Iteration 266: y_pred: 1.7999998969378668, loss: 1.0306213327204716e-07\n",
            "Iteration 267: y_pred: 1.7999999033586818, loss: 9.664131828124312e-08\n",
            "Iteration 268: y_pred: 1.7999999093794767, loss: 9.062052330754966e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNoKsPfiHlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d4c62ea-a176-49a6-fb2a-e4034dc65f73"
      },
      "source": [
        "pred(X,model['W1'],model['W2'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7999999124596768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}